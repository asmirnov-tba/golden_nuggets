{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601d9a90-f9e1-4d0e-8136-5b89bd7e5dcc",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Language Models<br>\n",
    "   <span style=\"font-size: 24px;\">An Introduction to Parallel CPU Inferencing of HuggingFace Models in Vantage</span>\n",
    "       \n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738d7d9-c96f-4816-ba0d-c480ba1560e2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>Introduction</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "Hugging Face is a French-American company based in New York City that develops computation tools for building applications using machine learning. They are known for their <b>Transformers Library</b> which provides open-source implementations of transformer models for text, image, video, audio tasks including time-series. These models include well-known architectures like BERT and GPT. The library is compatible with PyTorch, TensorFlow, and JAX deep learning libraries. <br>\n",
    "    Deep Learning Models in HuggingFace are pretrained by users/open source outfits/companies on various types of data – NLP, Audio, Images, Videos etc. Most popular tool of choice by users is PyTorch (open source python library) which helps create a Deep Learning model from scratch or take an existing model, retrain/fine-tune (Transfer Learning) on new set of data to be published in HF. Models can be inferenced with CPUs and GPUs with slight performance improvement for smaller models.<br>\n",
    "</p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Why Vantage?</b></p>  \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>As many Hugging Face models are availble in ONNX Runtime, we can load them using the BYOM feature of Vantage and run them in Vantage. Because of Graph Optimizations on ONNX Runtime, there are proven benchmarks that show that inference with ONNX Runtime will be 20% faster than a native PyTorch model on a CPU. Vantage Parallelism on top of boosted ONNX Runtime inference can turn a Vantage system as effective as inference on GPUs. If we have a Vantage box with 72 AMPs, assuming the table is perfectly distributed, it will closely match the performance of a dedicated GPU and data never moves across the network saving time and I/O operations. As parallelism increases with number of AMPs, the model inference will complete faster in Teradata Vantage with the same amount of text data vs a GPU. We can of course quantize the model (change float8 weights to int8/int4) for inference on CPU to go even faster with some tradeoff with accuracy. However, If Model size goes up GPU advantage will widen – example LLM like LLama2 and costs will be disproportionate with GPU but for smaller models we can get comparable performance. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101f149-3076-4bab-aec5-5f1a638cfa44",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:20px;font-family:Arial;color:#00233c'>1. Configuring the environment</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>1.1 Install the required libraries</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6405d1-ea63-42e6-a012-2aedb45f0228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum\n",
      "  Downloading optimum-1.23.3-py3-none-any.whl (424 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.1/424.1 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 KB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 KB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from optimum) (1.24.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from optimum) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from optimum) (0.26.2)\n",
      "Collecting transformers>=4.29\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.9/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from optimum) (21.3)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.9/site-packages (from optimum) (2.0.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from sentence_transformers) (9.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from sentence_transformers) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from sentence_transformers) (1.11.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->optimum) (3.0.7)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (2.0.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (10.9.0.58)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch>=1.11->optimum) (3.1.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->optimum) (62.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->optimum) (0.37.1)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.11->optimum) (3.30.5)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.11->optimum) (18.1.8)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.1/436.1 KB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.29->optimum) (2024.9.11)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.9/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.10.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->optimum) (2.1.3)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-18.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->optimum) (0.3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->optimum) (1.2.1)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.9/242.9 KB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.12.0\n",
      "  Downloading yarl-1.17.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.8/320.8 KB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->optimum) (21.4.0)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch>=1.11->optimum) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->optimum) (2022.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->optimum) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 KB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, safetensors, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, tokenizers, aiohttp, transformers, datasets, sentence_transformers, optimum\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.28.1\n",
      "    Uninstalling transformers-4.28.1:\n",
      "      Successfully uninstalled transformers-4.28.1\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.1.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 optimum-1.23.3 propcache-0.2.0 pyarrow-18.0.0 safetensors-0.4.5 sentence_transformers-3.2.1 tokenizers-0.20.3 tqdm-4.67.0 transformers-4.46.2 xxhash-3.5.0 yarl-1.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c2d7b-bac0-404d-bab8-6ce396a83711",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Note: </b><i>Please restart the kernel after executing these two lines. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce275db-9aa2-424f-88bd-edc91f2dcf84",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678dd728-e926-41c8-8c95-1f9a371ed873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Standard libraries\n",
    "import getpass\n",
    "import warnings\n",
    "\n",
    "# Teradata libraries\n",
    "from teradataml import *\n",
    "display.max_rows = 5\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b218aee-06db-4377-b170-8b1423029bce",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>2. Connect to Vantage</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7fd4855-ac57-4241-9d12-024b0d2cbae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing setup ...\n",
      "Setup complete\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter password:  ··············\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logon successful\n",
      "Connected as: teradatasql://demo_user:xxxxx@host.docker.internal/dbc\n",
      "Engine(teradatasql://demo_user:***@host.docker.internal)\n"
     ]
    }
   ],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host='host.docker.internal', username='demo_user', password=password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ccda78-2923-49ed-873d-58183d725c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql(\"SET query_band='DEMO=Language_Model_Init_Python.ipynb;' UPDATE FOR SESSION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b464041-461c-46dd-a18e-681878df2987",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c49645f4-2888-41f9-a03b-6a9628d1a6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have:  #databases=0 #tables=0 #views=0  You have used 0.8 MB of 30,678.3 MB available - 0.0%  ... Space Usage OK\n",
      " \n",
      "   Database Name                  #tables  #views     Avail MB      Used MB\n",
      "   demo_user                            0       0  30,678.3 MB       0.8 MB \n"
     ]
    }
   ],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891059d-98b1-44ef-b956-5310fe9eac82",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>3. Creation of functions</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Below command will create the database and functions required for text summarization and embedding models using Huggingface PyTorch models in Vantage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9159dd-84bb-42e7-acbf-860c8154fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('commands.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for item in data['queries']:\n",
    "    try:\n",
    "        execute_sql(item['query'])\n",
    "    except Exception as e:\n",
    "        print(f\"The initialization steps have already been executed for this environment!\")\n",
    "        #print(f\"Error: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e1d32-6c9d-4076-97a3-e7742ed7c579",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>4. HuggingFace Model installation</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the below steps we will download and install the HuggingFace Model in Vantage.</p> \n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.1 Download the Model using Optium utility</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will be using <a href = 'https://huggingface.co/BAAI/bge-small-en-v1.5'>BAAI/bge-small-en-v1.5</a><br> The bge-small-en model is a small-scale English text embedding model developed by BAAI (Beijing Academy of Artificial Intelligence) as part of their FlagEmbedding project.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "519fafdd-099f-4b6f-902d-8fb9034bf372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|████████████████████████████| 349/349 [00:00<00:00, 43.1kB/s]\n",
      "config_sentence_transformers.json: 100%|███████| 124/124 [00:00<00:00, 19.7kB/s]\n",
      "README.md: 100%|███████████████████████████| 94.8k/94.8k [00:00<00:00, 4.98MB/s]\n",
      "sentence_bert_config.json: 100%|█████████████| 52.0/52.0 [00:00<00:00, 21.5kB/s]\n",
      "config.json: 100%|██████████████████████████████| 743/743 [00:00<00:00, 325kB/s]\n",
      "model.safetensors: 100%|██████████████████████| 133M/133M [00:00<00:00, 136MB/s]\n",
      "tokenizer_config.json: 100%|███████████████████| 366/366 [00:00<00:00, 41.0kB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 16.9MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 711k/711k [00:00<00:00, 41.7MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 125/125 [00:00<00:00, 64.2kB/s]\n",
      "1_Pooling/config.json: 100%|███████████████████| 190/190 [00:00<00:00, 23.5kB/s]\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Weight deduplication check in the ONNX export requires accelerate. Please install accelerate to run it.\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --opset 16 --trust-remote-code --task sentence-similarity -m BAAI/bge-small-en-v1.5 bge-small-en-v1.5-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0b79d-5f7b-4ed0-9c4a-82942c94c0af",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.2 Model Preparation</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the below steps we will fix dynamic dims, fix versions for compatibility, etc and prepare the model to load in Vantage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524ca739-001a-49e4-808b-88e374fd4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as rt\n",
    "\n",
    "import transformers\n",
    "from onnxruntime.tools.onnx_model_utils import *\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "import teradataml as tdml\n",
    "\n",
    "op = onnx.OperatorSetIdProto()\n",
    "op.version = 16\n",
    "\n",
    "model = onnx.load('bge-small-en-v1.5-onnx/model.onnx')\n",
    "\n",
    "#to be sure that we have compatible opset and IR version\n",
    "model_ir8 = onnx.helper.make_model(model.graph, ir_version = 8, opset_imports = [op]) \n",
    "\n",
    "\n",
    "# fixing the variable dim sizes in our mode\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"batch_size\", 1) \n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"sequence_length\", 512)\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"Divsentence_embedding_dim_1\", 384)\n",
    "\n",
    "\n",
    "#remove useless token_embeddings output from the model\n",
    "for node in model_ir8.graph.output:\n",
    "    if node.name == \"token_embeddings\":\n",
    "        model_ir8.graph.output.remove(node)\n",
    "\n",
    "#saving the model\n",
    "onnx.save(model_ir8, 'bge-small-en-v1.5-onnx/model_fixed.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aad59a-c5f0-4e55-b915-650114098bc4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.2 Model Preparation</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Checking that everything works with ONNX format localy.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da55982-2380-4b61-bc27-55bc4e847bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = 'How is the weather today?'\n",
    "sentences_2 = 'What is the current weather like today?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7155d04b-92d6-445f-8c1c-67ccb5de8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./bge-small-en-v1.5-onnx\")\n",
    "predef_sess = rt.InferenceSession(\"bge-small-en-v1.5-onnx/model_fixed.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ccf9f13-2d81-4df2-b3b6-d8aa5ddb5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer(sentences_1, max_length = 512, padding='max_length' )\n",
    "\n",
    "result = predef_sess.run(None,     {\"input_ids\": [enc.input_ids], \n",
    "     \"attention_mask\": [enc.attention_mask]})\n",
    "\n",
    "enc2 = tokenizer(sentences_2, max_length = 512, padding='max_length' )\n",
    "\n",
    "result2 = predef_sess.run(None,     {\"input_ids\": [enc2.input_ids], \n",
    "     \"attention_mask\": [enc2.attention_mask]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d92421-e70b-4a19-9109-50b917eedced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9186]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "print(cos_sim(result[0][0], result2[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6f9c89-253c-49ec-beec-4db6de11cea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9186]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "embeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\n",
    "embeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\n",
    "\n",
    "print(cos_sim(embeddings_1, embeddings_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033ea38-677b-4e43-b301-8ada166d0233",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233c'><b>4.3 Deploy Model and Tokenizer</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In above steps, we have checked that the model is working fine in Onnx format. Now we will deploy the model and tokenizer in database.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d42f1694-bf8b-46f4-a9ce-2b21854e8517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the model table 'embeddings_models' as it does not exist.\n",
      "Model is saved.\n",
      "Created the model table 'embeddings_tokenizers' as it does not exist.\n",
      "Model is saved.\n"
     ]
    }
   ],
   "source": [
    "model_ids = ['bge-small-en-v1.5', 'bge-small-en-v1.5']\n",
    "model_files = ['bge-small-en-v1.5-onnx/model_fixed.onnx', 'bge-small-en-v1.5-onnx/tokenizer.json']\n",
    "table_names = ['embeddings_models', 'embeddings_tokenizers']\n",
    "\n",
    "for model_id, model_file, table_name in zip(model_ids, model_files, table_names):\n",
    "    try:\n",
    "        save_byom(model_id = model_id, model_file = model_file, table_name = table_name)\n",
    "    except Exception as e:\n",
    "        # if our model exists, delete and rewrite\n",
    "        if str(e.args).find('TDML_2200') >= 1:\n",
    "            print(f\"{table_name.split('_')[1][:-1]} already exists in the database\")\n",
    "            user_conformation = input(f\"Do you want to reload the {table_name.split('_')[1][:-1]} (y/n)?\")\n",
    "            if user_conformation.lower() == 'y':\n",
    "                delete_byom(model_id = model_id, table_name = table_name)\n",
    "                save_byom(model_id = model_id, model_file = model_file, table_name = table_name)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to save the {table_name.split('_')[1][:-1]} '{model_id}' in '{table_name}' due to the following error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda5b4d-772e-448c-9467-6f52bb8acd1c",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>5. Next Steps</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> Now we have initialized and loaded the model into Vantage.  Now the notebooks listed below can be executed.\n",
    "<ul style = 'font-size:16px;font-family:Arial;color:#00233C'> \n",
    "       <li>Semantic Similarity:Does Embeddings on CFPB complaints and uses TD_VECTORDISTANCE to find complaints that match some theme or topic <a href = './Semantic_Similarity_Python.ipynb'>Semantic Similarity </a></li> \n",
    "     <li>Semantic Clustering:Does Embeddings on CFPB complaints and uses K-MEANS to cluster and does Post-hoc explanations/topic detection on semantic clusters found. <a href = './Semantic_Clustering_Python.ipynb'>Semantic Clustering </a></li> \n",
    "     <li>RAG Notebook for TD Catalog:Does a dump of TD Catalog Metadata on a table. Does embeddings on both Metadata + language model prompt query. Does Semantic Similarity search of Top N Chunks and hands it off to a LLM to answer the prompt.<a href = './RAG_and_Bedrock_Querycatalogue.ipynb'>RAG and Bedrock to query Catalogue </a></li> \n",
    "     <li>RAG Notebook for SEC-10K PDF:Demo with some PDF parsing and chunking with a Teradata SEC-10K PDF, creates embedding and uses language model to answer prompts <a href = './RAG_and_Bedrock_QueryPDF.ipynb'>RAG and Bedrock to query Pdf </a></li> \n",
    "      \n",
    "</ul>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49407083-7481-463c-8a1f-2762702b78b2",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#f9f9f9; border-bottom:3px solid #00233C\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2024. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
